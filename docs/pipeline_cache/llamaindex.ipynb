{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama_index==0.11.4\n",
    "# !pip install PyYAML\n",
    "# !pip install docx2txt==0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Llama Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will learn what the architecture of a basic RAG system built by Llama index will consist of. You can create a basic RAG application after reading this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we create a Document object with input text - a string, and metadata - a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes\n",
    "\n",
    "Creating Documents is quite simple, however, the data in the document is still raw data, so how can raw data be converted into a format that LLM can process and infer effectively. The main solution is Nodes, which are smaller contents extracted from the document, the purpose of which is to divide the document into smaller parts for easier management.\n",
    "\n",
    "Nodes avoid the situation of exceeding the limit of the prompt that the model allows. For example, when there is a 100-page ebook, we do not use all the data in it directly into the prompt for LLM to process, because it will exceed the input limit of the model. Moreover, this method has the disadvantage of high processing costs, we have to pay more for using the API, the accuracy of the answer is also not guaranteed because when the input is such a long text, our prompt does not focus on any specific information, leading to the model not understanding and giving the correct answer. In addition, we can establish relationships between nodes.\n",
    "\n",
    "In Llamaindex, we can create nodes for text data using the TextNode class. The syntax used is:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: 9a3bea69-562a-4e9f-a3d6-f19758afd867\n",
      "Text: MENTAL\n",
      "Node ID: 99652ecc-6662-4c4a-8318-7fd334a64d17\n",
      "Text: CARE Team\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "text = \"MENTAL CARE Team\"\n",
    "doc = Document(\n",
    "        text=text\n",
    "        )\n",
    "\n",
    "node1 = TextNode(text=doc.text[:6])\n",
    "node2 = TextNode(text=doc.text[7:])\n",
    "\n",
    "print(node1)\n",
    "print(node2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create nodes automatically using TokenTextSplitter, the syntax is:\n",
    "\n",
    "```Python\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "TokenTextSplitter(\n",
    "chunk_size,\n",
    "chunk_overlap,\n",
    "separator,\n",
    ")\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "- chunk_size: The size of each text chunk. This is the number of tokens (words or characters) in each chunk.\n",
    "\n",
    "- chunk_overlap: The number of tokens that will be duplicated between consecutive chunks. This means that a part of the previous chunk will be repeated in the next chunk.\n",
    "\n",
    "- separator: The character or string of characters used to separate text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata length (2) is close to chunk size (18). Resulting chunks are less than 50 tokens. Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n",
      "Node ID: 4ae68656-8eb3-4144-856a-80da8016e92a\n",
      "Text: It's sunny today, I went to eat ice cream, it's so cold\n",
      "Node ID: 92498115-9ae5-4cc3-877a-31b5e03f7f21\n",
      "Text: it's so cold my teeth are freezing!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "text = \"It's sunny today, I went to eat ice cream, it's so cold my teeth are freezing!\"\n",
    "doc = Document(text=text)\n",
    "splitter = TokenTextSplitter(\n",
    "    chunk_size=18,\n",
    "    chunk_overlap=5,\n",
    "    separator= \" \"\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents([doc])\n",
    "\n",
    "for node in nodes:\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we call the TokenTextSplitter class from the node_parser module in the llama_index.core package. Next, we create a doc object from the text, then\n",
    "create a splitter object from the TokenTextSplitter class with the properties chunk_size, chunk_overlap, separator. To create nodes, we call the get_nodes_from_documents method from the splitter object we just created. So our nodes have been automatically created. A warning appears saying that we will have problems when our chunk_size is too small, smaller or not too different from the size of the metadata. Because then the metadata will take up most of the data of the node while the actual data is very little.\n",
    "\n",
    "Creating nodes does not stop at splitting the document, we can establish relationships between nodes, in addition, there are some more advanced node transformation methods that we will learn in the next section. Here is how we create relationships between two nodes, there are 5 relationships between nodes:\n",
    "\n",
    "In the system of dividing text into small segments (nodes) as used in the `llama_index` library, the relationships between nodes can be described as follows:\n",
    "\n",
    "1. **SOURCE**:\n",
    "- This node is the source document. This is the entire original text from which the other nodes are split.\n",
    "\n",
    "- For example: If the original text is \"It's sunny today, I went to eat ice cream, my teeth are cold!\", then the `SOURCE` node will contain this entire text.\n",
    "\n",
    "2. **PREVIOUS**:\n",
    "- This node is the previous node in the document. It represents the text immediately before the current node.\n",
    "\n",
    "- For example: If there are two consecutive nodes, the second node will have a `PREVIOUS` link pointing to the first node.\n",
    "\n",
    "3. **NEXT**:\n",
    "- This node is the next node in the document. It represents the text that follows the current node.\n",
    "\n",
    "- For example, if there are two consecutive nodes, the first node will have a `NEXT` link pointing to the second node.\n",
    "\n",
    "4. **PARENT**:\n",
    "- This node is the parent node in the document. It represents a larger text that contains the current node.\n",
    "\n",
    "- For example, if a large document is divided into smaller paragraphs, and each of these smaller paragraphs is divided into smaller paragraphs, then a node can have a parent node that contains it.\n",
    "\n",
    "5. **CHILD**:\n",
    "- This node is a child node in the document. It represents a smaller text that is contained within the current node.\n",
    "\n",
    "- For example, if a large text is divided into smaller paragraphs, then each smaller paragraph will be a child node of the large node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc ID: 0d10e863-0b95-43ce-ae23-186998cad8be\n",
      "Text: It's sunny today, I went to eat ice cream, it's so cold my teeth\n",
      "are freezing!\n",
      "Node ID: 7a4c8d96-3303-46c9-a982-752a7cd675c6\n",
      "Text: It's sunny today, I went to eat ice cream, it's so cold\n",
      "Node ID: 2a522af9-bd71-41ce-adf5-c41f603d2314\n",
      "Text: it's so cold my teeth are freezing!\n"
     ]
    }
   ],
   "source": [
    "print(doc)\n",
    "print(nodes[0])\n",
    "print(nodes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextNode(id_='7a4c8d96-3303-46c9-a982-752a7cd675c6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='0d10e863-0b95-43ce-ae23-186998cad8be', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='b1526564d45dbcefc0d21df5115c0273ae46be71605c670ea1594b01221dcb25'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='2a522af9-bd71-41ce-adf5-c41f603d2314', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='ad357d0f99e2c94faad156ee88603b8563023dda17192c3a90dabe953d616d1f')}, text=\"It's sunny today, I went to eat ice cream, it's so cold\", mimetype='text/plain', start_char_idx=0, end_char_idx=55, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='2a522af9-bd71-41ce-adf5-c41f603d2314', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='0d10e863-0b95-43ce-ae23-186998cad8be', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='b1526564d45dbcefc0d21df5115c0273ae46be71605c670ea1594b01221dcb25'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='7a4c8d96-3303-46c9-a982-752a7cd675c6', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='7989d88830163286446b05ad76b47dd2f4d9cf0550b7ce47c8eaaa9d8eb197aa')}, text=\"it's so cold my teeth are freezing!\", mimetype='text/plain', start_char_idx=43, end_char_idx=78, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='0d10e863-0b95-43ce-ae23-186998cad8be', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='b1526564d45dbcefc0d21df5115c0273ae46be71605c670ea1594b01221dcb25'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='2a522af9-bd71-41ce-adf5-c41f603d2314', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='ad357d0f99e2c94faad156ee88603b8563023dda17192c3a90dabe953d616d1f')}\n",
      "{<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='0d10e863-0b95-43ce-ae23-186998cad8be', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='b1526564d45dbcefc0d21df5115c0273ae46be71605c670ea1594b01221dcb25'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='7a4c8d96-3303-46c9-a982-752a7cd675c6', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='7989d88830163286446b05ad76b47dd2f4d9cf0550b7ce47c8eaaa9d8eb197aa')}\n"
     ]
    }
   ],
   "source": [
    "print(nodes[0].relationships)\n",
    "print(nodes[1].relationships)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The part of establishing relationships between nodes, these nodes will usually be created automatically, you can see in the example above, the relationships have been created automatically. We can see that node 1 has a SOURCE node as doc, has a NEXT relationship with node 2. Similarly, node 2 has a PREVIOUS relationship with node 1. In general, we just need to know that these relationships are very important, helping to query more accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "After you enter data into the system, LlamaIndex will help you index the data into an easily accessible structure. This process usually involves creating vector embeddings and storing them in a specialized database called a vector store. Indexing helps organize data so that it is easy to search and retrieve later.\n",
    "\n",
    "Llama Index supports many different types of indexes such as SummaryIndex, VectorStoreIndex, TreeIndex, KnowledgeGraphIndex, depending on each case, we will choose the appropriate type. In general, these types of indexes all perform the functions of creating indexes, adding new nodes, and querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata length (2) is close to chunk size (20). Resulting chunks are less than 50 tokens. Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n",
      "index 1 IndexList(index_id='166953ab-ab4f-412b-b3f5-2cdf40ae2d71', summary=None, nodes=['30aad8eb-e8d1-42bb-a5b4-978daf8ee7df'])\n",
      "index 2 IndexList(index_id='5798db81-5a25-4508-b206-e215defd2638', summary=None, nodes=['6df79211-d0b9-47e1-8d55-a5783250884f'])\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document, SummaryIndex\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "text = \"The fat cat was lying by the window. I wished I could be like that.\"\n",
    "doc = Document(\n",
    "    text=text\n",
    "    )\n",
    "\n",
    "splitter = TokenTextSplitter(\n",
    "    chunk_size=20,\n",
    "    chunk_overlap=5,\n",
    "    separator= \" \"\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents([doc])\n",
    "index = SummaryIndex(nodes)\n",
    "index2 = SummaryIndex.from_documents([doc])\n",
    "\n",
    "print(\"index 1\", index.index_struct)\n",
    "print(\"index 2\", index2.index_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2 = SummaryIndex.from_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndexList(index_id='166953ab-ab4f-412b-b3f5-2cdf40ae2d71', summary=None, nodes=['30aad8eb-e8d1-42bb-a5b4-978daf8ee7df'])\n"
     ]
    }
   ],
   "source": [
    "print(index.index_struct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we create an index from nodes using SummayIndex, we can see the structure of the index via index_struct. So the index is ready to be retrieved, from the beginning until now we created Doc, created Node, created Index is only for the purpose of retrieving information from data, right? To retrieve data in the index, Llama Index supports many different retrieval tools, but the simplest here is to create a query_engine object from index.as_query_engine(), which means we create a question and answer engine from the index. Usage is also very simple, just call the query_engine.query() method with the parameter being the content of the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "respone = query_engine.query(\"what is fat cat doing?\")\n",
    "print(respone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we get an error because we have not set the OPEN AI API key so we cannot use query_engine. Now let's add the key to fix the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"sk-proj-your-openai-api-key\"\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we set the api_key from open api, in addition, we set the LLM model that we use as gpt-4o-mini, with temperature as 0.2. Here, temperature has a value from 0 to 2, the higher the value, the more creative and random the answer. When the value is lower or equal to 0, the result will be more fixed, that is, we ask a question many times, the answer is the same. In general, depending on the purpose of use, we will adjust the number appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fat cat was lying by the window.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "respone = query_engine.query(\"what is fat cat doing?\")\n",
    "print(respone)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aio_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
